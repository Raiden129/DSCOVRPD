{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D_R(nn.Module):\n",
    "    def __init__(self, input_dim, input_len, total_level, current_level, dropout, enc_hidden, output_len, K_IMP):\n",
    "        super().__init__()\n",
    "        self.current_level = current_level\n",
    "        self.input_len = input_len\n",
    "        self.input_dim = input_dim\n",
    "        self.enc_hidden = enc_hidden\n",
    "        self.output_len = output_len\n",
    "        self.dropout = dropout\n",
    "        self.K_IMP = K_IMP\n",
    "        self.total_level = total_level\n",
    "\n",
    "        self.MCD = MCD(K_IMP, kernel_size=(1, 3), soft_max=False)\n",
    "\n",
    "        if current_level == 0:\n",
    "            pass\n",
    "        else:\n",
    "            self.branch_slelect = nn.Sequential(\n",
    "                nn.Linear(input_len, enc_hidden),\n",
    "                nn.BatchNorm2d(enc_hidden, affine=True),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Linear(enc_hidden, 2),\n",
    "            )\n",
    "\n",
    "            self.reconstruct_proj_left = nn.Sequential(\n",
    "                nn.Linear(input_len, enc_hidden),\n",
    "                nn.BatchNorm1d(enc_hidden, affine=True),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(self.dropout),\n",
    "                nn.Linear(enc_hidden, input_len),\n",
    "                nn.BatchNorm1d(enc_hidden, affine=True),\n",
    "                nn.LeakyReLU()\n",
    "            )\n",
    "\n",
    "            self.reconstruct_proj_right = nn.Sequential(\n",
    "                nn.Linear(input_len, enc_hidden),\n",
    "                nn.BatchNorm1d(enc_hidden, affine=True),\n",
    "                nn.LeakyReLU(),\n",
    "                nn.Dropout(self.dropout),\n",
    "                nn.Linear(enc_hidden, input_len),\n",
    "                nn.BatchNorm1d(enc_hidden, affine=True),\n",
    "                nn.LeakyReLU()\n",
    "            )\n",
    "\n",
    "            self.EMDNet_Tree_left = D_R(\n",
    "                input_dim, input_len, total_level, current_level - 1, dropout, enc_hidden, output_len, K_IMP)\n",
    "            self.EMDNet_Tree_right = D_R(\n",
    "                input_dim, input_len, total_level, current_level - 1, dropout, enc_hidden, output_len, K_IMP)\n",
    "\n",
    "            if current_level == total_level:\n",
    "                self.forecast_proj1 = nn.ModuleList(\n",
    "                    [nn.Linear(self.input_len, self.enc_hidden)\n",
    "                     for i in range(2 ** total_level * self.K_IMP)]\n",
    "                )\n",
    "                self.activate = nn.LeakyReLU()\n",
    "                self.forecast_proj2 = nn.ModuleList(\n",
    "                    [nn.Linear(self.enc_hidden, self.input_len)\n",
    "                     for i in range(2 ** total_level * self.K_IMP)]\n",
    "                )\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    def decompose_MCD(self, x):\n",
    "        x = self.MCD(x)\n",
    "        return x  # (B, N, K, T)\n",
    "\n",
    "    def reconstruct(self, x_imf):  # (B, N, K, T)\n",
    "        select_feature = self.branch_slelect(x_imf)  # (B, N, K, 2)\n",
    "        hard_class = gumbel_softmax(select_feature, hard=False)\n",
    "        x_imf = x_imf.permute(0, 1, 3, 2)  # (B, N, T, K)\n",
    "        x_summed = torch.matmul(x_imf, hard_class)  # (B, N, T, 2)\n",
    "\n",
    "        x_left = self.reconstruct_proj_left(\n",
    "            x_summed[:, :, :, 0])  # (B, N, T) --> # (B, N, T)\n",
    "        x_right = self.reconstruct_proj_right(x_summed[:, :, :, 1])\n",
    "\n",
    "        return x_left, x_right\n",
    "\n",
    "    def forecast(self, total_imf):\n",
    "        for i in range(2 ** self.total_level * self.K_IMP):\n",
    "            y_current_imf = self.forecast_proj2[i](\n",
    "                self.activate(\n",
    "                    self.forecast_proj1[i](total_imf[:, :, i, :])\n",
    "                )\n",
    "            )\n",
    "            y_current_imf = y_current_imf.unsqueeze(2)\n",
    "            if i == 0:\n",
    "                y_imf = y_current_imf\n",
    "            else:\n",
    "                y_imf = torch.cat((y_imf, y_current_imf), axis=2)\n",
    "\n",
    "        return y_imf  # (B, N, 2^level*K, T=input_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.current_level == 0:\n",
    "            x_imf = self.decompose_MCD(x)\n",
    "            return x_imf  # (B, N, K, T)\n",
    "        else:\n",
    "            x_imf = self.decompose_MCD(x)\n",
    "            x_left, x_right = self.reconstruct(x_imf)\n",
    "            imf_left = self.EMDNet_Tree_left(x_left)\n",
    "            imf_right = self.EMDNet_Tree_left(x_right)\n",
    "            total_imf = torch.cat([imf_left, imf_right], dim=2)\n",
    "\n",
    "            if self.current_level == self.total_level:\n",
    "                y = self.forecast(total_imf)\n",
    "                return y\n",
    "            else:\n",
    "                return total_imf\n",
    "\n",
    "\n",
    "class IFNet(nn.Module):\n",
    "    def __init__(self, output_len, input_len, input_dim=18, dec_hidden=18, num_heads=1):\n",
    "        super(IFNet, self).__init__()\n",
    "        self.multi_att = MultiHeadSelfAttention(\n",
    "            dim_in=input_len, dim_k=dec_hidden * num_heads, dim_v=dec_hidden * num_heads, num_heads=num_heads)\n",
    "        self.predict = nn.Sequential(\n",
    "            nn.Linear(dec_hidden * num_heads, input_len),\n",
    "            nn.BatchNorm1d(input_dim, affine=True),  # This line uses input_dim\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(input_len, output_len),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.multi_att(x)\n",
    "        x = torch.sum(x, 2)\n",
    "        x = self.predict(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DPAD(nn.Module):\n",
    "    def __init__(self, input_dim, input_len, num_levels, dropout, enc_hidden, dec_hidden, output_len, K_IMP, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_levels = num_levels\n",
    "        self.K_IMP = K_IMP\n",
    "        self.D_R_D = D_R(\n",
    "            input_dim=input_dim,\n",
    "            input_len=input_len,\n",
    "            total_level=num_levels - 1,\n",
    "            current_level=num_levels - 1,\n",
    "            dropout=dropout,\n",
    "            enc_hidden=enc_hidden,\n",
    "            output_len=output_len,\n",
    "            K_IMP=K_IMP\n",
    "        )\n",
    "        self.IF = IFNet(\n",
    "            output_len=output_len,\n",
    "            input_len=input_len,\n",
    "            input_dim=input_dim,\n",
    "            dec_hidden=dec_hidden,\n",
    "            num_heads=num_heads\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.D_R_D(x)  # (B, N, 2^(level-1)*K, T=input_len)\n",
    "        x = self.IF(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DPAD_ATT(nn.Module):\n",
    "    def __init__(self, output_len, input_len, input_dim=18, enc_hidden=18, dec_hidden=18, num_levels=3, dropout=0.5,\n",
    "                 K_IMP=6, RIN=0, num_heads=1):\n",
    "        super(DPAD_ATT, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.enc_hidden = enc_hidden\n",
    "        self.dec_hidden = dec_hidden\n",
    "        self.num_levels = num_levels\n",
    "        self.dropout = dropout\n",
    "        self.K_IMP = K_IMP\n",
    "        self.RIN = RIN\n",
    "\n",
    "        print(f\"Initializing DPAD with input_dim: {input_dim}, input_len: {input_len}\")\n",
    "\n",
    "        self.DPAD = DPAD(\n",
    "            input_dim=self.input_dim,\n",
    "            input_len=self.input_len,\n",
    "            num_levels=self.num_levels,\n",
    "            dropout=self.dropout,\n",
    "            enc_hidden=self.enc_hidden,\n",
    "            dec_hidden=self.dec_hidden,\n",
    "            output_len=output_len,  # This should match your target sequence length\n",
    "            K_IMP=self.K_IMP,\n",
    "            num_heads=num_heads\n",
    "        )\n",
    "\n",
    "        if self.RIN:\n",
    "            self.affine_weight = nn.Parameter(torch.ones(1, 1, input_dim))\n",
    "            self.affine_bias = nn.Parameter(torch.zeros(1, 1, input_dim))\n",
    "\n",
    "        self.final_linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "        if self.RIN:\n",
    "            means = x.mean(1, keepdim=True).detach()\n",
    "            x = x - means\n",
    "            stdev = torch.sqrt(\n",
    "                torch.var(x, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            x /= stdev\n",
    "            x = x * self.affine_weight + self.affine_bias\n",
    "\n",
    "        x = x.permute(0, 2, 1)  # (B, N, T)\n",
    "        print(f\"Shape after permute: {x.shape}\")\n",
    "        x = self.DPAD(x)\n",
    "        print(f\"Shape after DPAD: {x.shape}\")\n",
    "        x = x.permute(0, 2, 1)  # (B, T, N)\n",
    "        print(f\"Final output shape: {x.shape}\")\n",
    "        x = self.final_linear(x)\n",
    "\n",
    "        if self.RIN:\n",
    "            x = x - self.affine_bias\n",
    "            x = x / (self.affine_weight + 1e-10)\n",
    "            x = x * stdev\n",
    "            x = x + means\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
